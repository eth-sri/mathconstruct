llm_costs = {
    "gpt-4o-mini.*": (0.15, 0.6),
    "gpt-4o.*": (2.5, 10),
    "o1.*": (15, 60),
    "o1-preview.*": (15, 60),
    "o1-mini.*": (3, 12),
    "o3-mini.*": (1.1, 4.4),
    "gpt-4-turbo.*": (10, 30),
    "gpt-4.*": (30, 60),
    "gpt-4-32k": (60, 120),
    "gpt-3.5-turbo.*": (0.5, 1.5),
    "claude-3-5-sonnet.*": (3, 15),
    "claude-3.5-sonnet.*": (3, 15),
    "claude-3-5-haiku.*": (0.8, 4),
    "claude-3.5-haiku.*": (0.8, 4),
    "claude-3-opus.*": (15, 75),
    "gemini-1.5-flash.*": (0.08,0.3),
    "gemini-1.5-pro.*": (1.2, 5),
    ".*deepseek-reasoner.*": (0.55, 2.19),
    ".*deepseek-chat.*": (0.14, 0.28),
    ".*deepseek/deepseek-r1-distill-llama-70b.*": (0.8, 0.8),
    ".*deepseek-ai/DeepSeek-R1-Distill-Llama-70B.*": (2, 2),
    ".*deepseek-r1.*": (8,8),
    ".*deepseek-chat.*": (0.85, 0.9),
    ".*DeepSeek-R1.*": (7, 7),
    ".*DeepSeek-V3.*": (1.25, 1.25),
    ".*meta-llama/Llama-3.3-70B-Instruct-Turbo.*": 0.88,
    "deepseek-ai/DeepSeek-V3": 1.25,
    ".*Meta-Llama-3.1-405B-Instruct-Turbo.*": 3.5,
    ".*Meta-Llama-3.1-8B-Instruct-Turbo-128K.*": 0.18,
    "Qwen/QwQ-32B-Preview": 1.2,
    ".*72B.*": 1.2,
    ".*32B.*": 0.8,
    ".*70B.*": 0.88,
    ".*7B.*": 0.3,
    ".*27b.*": 0.8,
    ".*9b.*": 0.3
}
